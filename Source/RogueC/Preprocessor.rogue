class Preprocessor
  GLOBAL PROPERTIES
    definitions = StringTable<<Token[]>>()

  GLOBAL METHODS
    method define( name:String, definition:String )
      define( name, Tokenizer().tokenize("[Command Line]",definition) )

    method define( name:String, tokens:Token[] )
      definitions[ name ] = tokens

    method logicalize( term: Value, loose = false: Logical )->Logical
      if (not loose)
        if (term == false) return false
        if (term == 0) return false
        if (term.is_null) return false
        if (term == "") return false
        return true
      endIf
      if (term.is_number)
        return term != 0
      endIf
      local v = term->String.to_lowercase
      return not (v == "" or v == "0" or v == "false" or v == "null" or v == "no")

    method valueize( tok: Token )->Value
      which (tok.type)
        case TokenType.keyword_true
          return true
        case TokenType.keyword_false
          return false
        case TokenType.literal_int32
          return tok->Int32
        case TokenType.literal_int64
          return tok->Int64
        case TokenType.literal_real64
          return tok->Real64
        case TokenType.keyword_false
          return false
        case TokenType.keyword_true
          return true
        case TokenType.keyword_null
          return null
        case TokenType.literal_character,TokenType.literal_string
          return tok->String
      endWhich
      return null

    method valueize( toks: Token[] )->Value
      if (toks.count == 2 and toks[0].type == TokenType.symbol_minus)
        local v = valueize(toks[1])
        if (v.is_number)
          return v * -1
        endIf
        return " ".join(toks.mapped<<String>>( (x)=>x ))
      elseIf (toks.count == 0)
        # Count "$define FOO" as true
        return true
      elseIf (toks.count != 1)
        # Just string it all out.
        return " ".join(toks.mapped<<String>>( (x)=>x ))
      endIf
      local r = valueize(toks[0])
      if (r is null)
        return toks[0]->String
      endIf
      return r

  PROPERTIES
    parser : Parser  # the parser that will be parsing these tokens later
    reader : PreprocessorTokenReader
    tokens : Token[]

    cur_module : String
    metablock_stack = Token[]

  METHODS
    method init( parser )

    method process( tokens )->Token[]
      reader = PreprocessorTokenReader( tokens )
      tokens = Token[]( (tokens.count * 1.1)->Int32 )
      process( true, 0, false )

      return tokens

    # -------------------------------------------------------------------------

    method consume( type:TokenType )->Logical
      if (reader.peek.type is not type) return false
      reader.read
      return true

    method consume( identifier:String )->Logical
      local t = reader.peek
      if (t.type is not TokenType.identifier) return false
      if (t->String != identifier) return false
      reader.read
      return true

    method process( keep_tokens:Logical, depth:Int32, stop_on_eol:Logical )
      ++depth
      local repeat_string_concatenation = false
      while (reader.has_another)
        local t = reader.read

        if (t.is_directive)
          if (t.type is TokenType.directive_define)
            if (keep_tokens)
              local defined_word = reader.read_identifier_unfiltered
              local defined_tokens = Token[]()
              while (reader.has_another)
                if (reader.peek.type is TokenType.eol) escapeWhile
                defined_tokens.add( reader.read )
              endWhile
              define( defined_word, defined_tokens )
            else
              # Skip this directive due to conditional compilation - discard tokens to EOL
              while (reader.has_another)
                if (reader.peek.type is TokenType.eol) escapeWhile
                reader.read
              endWhile
            endIf
            nextIteration

          elseIf (t.type is TokenType.directive_defined)
            local defined_word = t->String
            if (keep_tokens)
              if (definitions.contains(defined_word))
                tokens.add( Token(TokenType.keyword_true).set_location(t) )
              else
                tokens.add( Token(TokenType.keyword_false).set_location(t) )
              endIf
            endIf
            nextIteration

          elseIf (t.type is TokenType.directive_error)
            local message_t = reader.read
            if (message_t.type is not TokenType.literal_string)
              throw t.error( "Literal string error message expected." )
            endIf
            if (keep_tokens)
              throw message_t.error( message_t->String )
            endIf
            nextIteration

          elseIf (t.type is TokenType.directive_include)
            local filepath_t = reader.read
            if (filepath_t.type is not TokenType.literal_string)
              throw reader.error( "Filepath expected." )
            endIf
            local is_optional = consume_optional
            if (keep_tokens)
              RogueC.include_source( t, filepath_t->String, &is_optional=is_optional )
            # else next literal string will be discarded anyway
            endIf
            nextIteration

          elseIf (t.type is TokenType.directive_elseIf)
            if (depth == 1) throw t.error( "Syntax error - $elseIf does not match any previous $if." )
            reader.push( t )
            return

          elseIf (t.type is TokenType.directive_else)
            if (depth == 1) throw t.error( "Syntax error - $else does not match any previous $if." )
            reader.push( t )
            return

          elseIf (t.type is TokenType.directive_endIf)
            if (depth == 1) throw t.error( "Syntax error - $endIf does not match any previous $if." )
            reader.push( t )
            return

          elseIf (t.type is TokenType.directive_if)
            local found_true = parse_logical_expression
            local single_line = not reader.next_is( TokenType.eol )

            if (found_true)
              process( keep_tokens, depth, single_line )
            else
              process( false, depth, single_line )
            endIf

            while (reader.peek.type is TokenType.directive_elseIf)
              reader.read
              local value = parse_logical_expression

              if (found_true)
                process( false, depth, single_line )
              else
                found_true = value
                if (value) process( keep_tokens, depth, single_line )
                else       process( false, depth, single_line )
              endIf
            endWhile

            if (reader.peek.type is TokenType.directive_else)
              reader.read
              if (found_true) process( false,       depth, single_line )
              else            process( keep_tokens, depth, single_line )
            endIf

            if (not single_line) must_consume( TokenType.directive_endIf )
            nextIteration

          elseIf (t.type is TokenType.directive_warning)
            local message_t = reader.read
            if (message_t.type is not TokenType.literal_string)
              throw t.error( "Literal string warning message expected." )
            endIf
            if (keep_tokens)
              Console.error.println message_t.warning( message_t->String )
            endIf
            nextIteration

          elseIf (t.type is TokenType.directive_module)
            if (keep_tokens) tokens.add( t )

            if (next_is(TokenType.identifier))
              if (keep_tokens) tokens.add( reader.peek )
              cur_module = reader.read_identifier
            else
              cur_module = null
            endIf

            if (next_is(TokenType.symbol_open_bracket))
              # [essential]
              if (keep_tokens)
                tokens.add( reader.read )
                while (reader.has_another)
                  tokens.add( reader.read )
                  if (tokens.last.type is TokenType.symbol_close_bracket) escapeWhile
                endWhile
              else
                reader.read
                while (reader.has_another)
                  local next_token = reader.read
                  if (next_token.type is TokenType.symbol_close_bracket) escapeWhile
                endWhile
              endIf
            endIf

            nextIteration

          elseIf (t.type is TokenType.meta_block)
            if (keep_tokens)
              metablock_stack.add( t )
              reader.local_definitions.add( StringTable<<Token[]>>() )
            endIf
            nextIteration

          elseIf (t.type is TokenType.meta_endBlock)
            if (keep_tokens)
              if (metablock_stack.is_empty) throw t.error( "$endBlock without preceding $block" )
              metablock_stack.remove_last
              reader.local_definitions.remove_last
            endIf
            nextIteration

          elseIf (t.type is TokenType.meta_localDefine)
            if (keep_tokens)
              local defined_word = reader.read_identifier_unfiltered
              local defined_tokens = Token[]()
              while (reader.has_another)
                if (reader.peek.type is TokenType.eol) escapeWhile
                defined_tokens.add( reader.read )
              endWhile
              if (reader.local_definitions.is_empty) reader.local_definitions.add( StringTable<<Token[]>>() )
              reader.local_definitions.last[ defined_word ] = defined_tokens
            else
              # Skip this directive due to conditional compilation - discard tokens to EOL
              while (reader.has_another)
                if (reader.peek.type is TokenType.eol) escapeWhile
                reader.read
              endWhile
            endIf
            nextIteration

          elseIf (t.type is TokenType.meta_moduleName)
            if (keep_tokens)
              if (cur_module) tokens.add( TokenType.literal_string.create_token(t,cur_module) )
              else            tokens.add( TokenType.literal_string.create_token(t,"") )
              repeat_string_concatenation = true
            endIf
            nextIteration

          endIf
        endIf

        if (t.type is TokenType.keyword_class or t.type is TokenType.keyword_routine or t.type is TokenType.keyword_enum)
          if (cur_module and reader.peek.type is TokenType.identifier)
            if (keep_tokens) tokens.add( t )
            t = reader.read
            local class_name = t->String
            Program.add_module_id( cur_module, class_name )
            if (keep_tokens)
              tokens.add( t.type.create_token(t,"$::$"(cur_module,class_name)) )
            endIf
            nextIteration
          endIf
        endIf

        if (t.type is TokenType.literal_string)
          while (reader.peek.type is TokenType.literal_string)
            t = t.type.create_token( t, t->String + reader.read->String )
          endWhile
        endIf

        if (keep_tokens) tokens.add( t )
        if (stop_on_eol and t.type is TokenType.eol) escapeWhile

      endWhile

      if (metablock_stack.count)
        throw metablock_stack.last.error( "Unclosed $block; no matching $endBlock found." )
      endIf

      if (repeat_string_concatenation)
        forEach (t in rewriter=tokens.rewriter)
          if (t.type is TokenType.literal_string)
            while (rewriter.has_another and rewriter.peek.type is TokenType.literal_string)
              t = t.type.create_token( t, t->String + rewriter.read->String )
            endWhile
          endIf
          rewriter.write( t )
        endForEach
      endIf

    method consume_optional->Logical
      if (not consume(TokenType.symbol_open_bracket)) return false
      local is_optional = false
      while (consume("optional")) is_optional = true
      if (not next_is(TokenType.symbol_close_bracket))
        throw reader.peek.error( "Keyword 'optional' or closing ']' expected." )
      endIf
      must_consume( TokenType.symbol_close_bracket )
      return is_optional

    method must_consume( type:TokenType )
      local message = "Expected '$'." (type.name)
      if (not reader.has_another) throw RogueError( message )
      local t = reader.read
      if (t.type is not type) throw t.error( message )

    method next_is( type:TokenType )->Logical
      return reader.next_is( type )

    method parse_logical_expression->Logical
      reader.push_unfiltered
      local r = parse_logical_or
      reader.pop_unfiltered
      return r

    method parse_logical_or->Logical
      return parse_logical_or( parse_logical_and )

    method parse_logical_or( lhs:Logical )->Logical
      if (consume(TokenType.keyword_or)) return parse_logical_or( parse_logical_and or lhs )
      return lhs

    method parse_logical_and->Logical
      return parse_logical_and( parse_logical_compare )

    method parse_logical_and( lhs:Logical )->Logical
      if (consume(TokenType.keyword_and)) return parse_logical_and( parse_logical_compare and lhs)
      return lhs

    method parse_logical_compare->Logical
      return parse_logical_compare( parse_logical_term )

    method parse_logical_compare( lhs:Value )->Logical
      # lhs is on the right so that we don't short circuit -- we need to parse everything one
      # way or another!
      if (consume(TokenType.symbol_eq)) return parse_logical_compare( parse_logical_term == lhs )
      if (consume(TokenType.symbol_ne)) return parse_logical_compare( parse_logical_term != lhs )
      if (consume(TokenType.symbol_ge)) return parse_logical_compare( parse_logical_term <= lhs )
      if (consume(TokenType.symbol_gt)) return parse_logical_compare( parse_logical_term <  lhs )
      if (consume(TokenType.symbol_le)) return parse_logical_compare( parse_logical_term >= lhs )
      if (consume(TokenType.symbol_lt)) return parse_logical_compare( parse_logical_term >  lhs )
      return logicalize(lhs)

    method parse_logical_term->Value
      loop
        local t = reader.peek
        if (consume(TokenType.keyword_not))
          local result = (not logicalize(parse_logical_term))
          return result
        endIf

        if (consume(TokenType.symbol_open_paren))
          local result = parse_logical_expression
          must_consume(TokenType.symbol_close_paren)
          return result
        endIf

        if (t.type is TokenType.symbol_minus)
          reader.read
          local v = parse_logical_term
          if (not v.is_number)
            throw t.error( "Syntax error: unexpected '-'." )
          endIf
          return v * -1
        endIf

        if (consume(TokenType.directive_defined))
          must_consume(TokenType.symbol_open_paren)
          local def = reader.peek
          must_consume(TokenType.identifier)
          local result = definitions.contains(def->String)
          must_consume(TokenType.symbol_close_paren)
          return result
        endIf

        if (consume(TokenType.identifier))
          if (t->String == "target")
            must_consume(TokenType.symbol_open_paren)
            local target = reader.peek
            must_consume(TokenType.literal_string)
            local result = RogueC.compile_targets[target->String]->Logical
            must_consume(TokenType.symbol_close_paren)
            return result
          endIf

          local maybe = false
          if (t->String == "optional")
            must_consume(TokenType.symbol_open_paren)
            maybe = true
            t = reader.peek
            must_consume(TokenType.identifier)
          endIf

          local name = t->String
          local v = false : Value
          local has_value = definitions.contains(name)
          if (has_value) v = valueize(definitions[name])
          if (v is null) throw reader.peek.error( "Definition '$' does not have a legal value for preprocessor expressions." (name) )

          if (reader.peek->String == "?")
            reader.read
            v = logicalize(v, &loose=true)
          endIf

          if (has_value == false and maybe == false)
            throw t.error( "Unknown preprocessor definition: '$'." (name) )
          endIf

          if (maybe) must_consume(TokenType.symbol_close_paren)

          return v
        endIf

        local v = valueize(t)
        if (v is null) throw reader.peek.error( "Syntax error in directive: '$'." (reader.peek) )
        reader.read
        return v
      endLoop

    method read_identifier->String
      local t = reader.peek
      if (t.type is not TokenType.identifier) throw t.error( "Identifier expected instead of '$'." (t) )
      return reader.read->String

    method reprocess( tokens )->Token[]
      # All the heavy lifting has been done.  Just join any string literals that may now be
      # adjacent after a new template instance.
      local rewriter = tokens.rewriter
      while (rewriter.has_another)
        local t = rewriter.read
        while (t.type is TokenType.literal_string and rewriter.has_another and rewriter.peek.type is TokenType.literal_string)
          t = t.type.create_token( t, t->String + rewriter.read->String )
        endWhile
        rewriter.write( t )
      endWhile

      return tokens
endClass

